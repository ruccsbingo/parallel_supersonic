// Copyright 2010 Google Inc. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

#include "supersonic/cursor/core/aggregate_groups.h"

namespace supersonic {


FailureOrVoid GroupAggregateCursor::ProcessInput() {
  if (reset_aggregator_in_processinput_) {
    reset_aggregator_in_processinput_ = false;
    key_->Reset();
    // Compacting GroupKeySet to release more memory. This is a workaround for
    // having (allocator_->Available() == 0) constantly, which would allow to
    // only process one block of input data per call to ProcessInput(). However,
    // freeing the underlying datastructures and building them from scratch many
    // times can be inefficient.
    // TODO(user): Implement a less aggressive solution to this problem.
    key_->Compact();
    aggregator_->Reset();
  }
  rowcount_t row_count = key_->size();

  // Process the input while not exhausted and memory quota not exceeded.
  // (But, test the condition at the end of the loop, to guarantee that we
  // process at least one chunk of the input data).
  do {
    // Fetch next block from the input.
    if (!PREDICT_TRUE(child_.Next(Cursor::kDefaultRowCount, false))) {
      PROPAGATE_ON_FAILURE(child_);
      if (!child_.has_data()) {
        if (child_.is_eos()) {
          input_exhausted_ = true;
        } else {
          DCHECK(child_.is_waiting_on_barrier());
          DCHECK(!reset_aggregator_in_processinput_);
          return Success();
        }
      }
      break;
    }
    // Add new rows to the key set.
    child_.truncate(key_->Insert(child_.view(), &inserted_keys_));
    if (child_.view().row_count() == 0) {
      // Failed to add any new keys.
      break;
    }
    row_count = key_->size();
    if (aggregator_->capacity() < row_count) {
      // Not enough space to hold the aggregate columns; need to reallocate.
      // But, if already over quota, give up.
      if (allocator_->Available() == 0) {
        // Rewind the input and ignore trailing rows.
        row_count = aggregator_->capacity();
        for (rowid_t i = 0; i < child_.view().row_count(); ++i) {
          if (inserted_keys_.row_ids()[i] >= row_count) {
            child_.truncate(i);
            break;
          }
        }
      } else {
        if (original_allocator_->Available() < allocator_->Available()) {
          THROW(new Exception(ERROR_MEMORY_EXCEEDED,
                              "Underlying allocator ran out of memory."));
        }
        // Still have spare memory; reallocate.
        rowcount_t requested_capacity = std::max(2 * aggregator_->capacity(),
                                                 row_count);
        if (!aggregator_->Reallocate(requested_capacity)) {
          // OOM when trying to make more room for aggregate columns. Rewind the
          // last input block, so that it is re-fetched by the next
          // ProcessInput, and break out of the loop, returning what we have up
          // to now.
          row_count = aggregator_->capacity();
          child_.truncate(0);
          break;
        }
      }
    }
    // Update the aggregate columns w/ new rows.
    PROPAGATE_ON_FAILURE(
        aggregator_->UpdateAggregations(child_.view(),
                                        inserted_keys_.row_ids()));
  } while (allocator_->Available() > 0);
  if (!input_exhausted_) {
    if (best_effort_) {
      if (row_count == 0) {
        THROW(new Exception(
            ERROR_MEMORY_EXCEEDED,
            StringPrintf(
                "In best-effort mode, failed to process even a single row. "
                "Memory free: %zd",
                allocator_->Available())));
      }
    } else {  // Non-best-effort.
      THROW(new Exception(
          ERROR_MEMORY_EXCEEDED,
          StringPrintf(
              "Failed to process the entire input. "
              "Memory free: %zd",
              allocator_->Available())));
    }
  }
  const View* views[] = { &key_->key_view(), &aggregator_->data() };
  result_projector_->Project(&views[0], &views[2], my_view());
  my_view()->set_row_count(row_count);
  result_.reset(*my_view());
  reset_aggregator_in_processinput_ = true;
  return Success();
}

// Hybrid group implementation classes and functions.

// Hybrid group aggregate uses disk-based sorting to allow processing more data
// than would fit in memory.
//
// The key steps of the algorithm are:
// - DISTINCT aggregation elimination
// - preaggregation with best-effort GroupAggregate
// - sorting the preaggregated data
// - combining preaggregated rows on duplicate keys
// - final aggregation using AggregateClusters.
//
// The motivation for eliminating DISINCT aggregations is threefold:
// - partial results of DISTINCT aggregations can't be easily combined,
// - building a whole set of unique values of an attribute for a single key can
//   take arbitrary amount of memory,
// - existing code for computing DISTINCT aggregations doesn't track the memory
//   needed for tracking sets of unique values,
// Here DISTINCT aggregations are eliminated by removing duplicate values in
// DISTINCT aggregated columns, so these aggregations can then be computed with
// ordinary, non-DISTINCT aggregating functions. This is done by adding DISTINCT
// aggregated columns to the preaggregation key. To get unique values for each
// DISTINCT aggregated column, each column needs to be processed separately. It
// is achieved by transforming the input in such a way that each DISTINCT
// aggregated column gets its copy of rows, where all other distinct aggregated
// columns are set to NULL. This is the job of BoundHybridGroupTransform. The
// approach used here relies heavily on the way NULLs are handled in
// aggregation, most notably:
// - NULLs in grouping key is treated as a distinct value,
// - except for COUNT(*) aggregating functions ignore NULL input values.
//
// The next step is to preaggregate the (possibly transformed) input data using
// best-effort GroupAggregate on a key extended with any DISTINCT aggregated
// columns. This is done to reduce the amount of data that will need to be
// sorted before final aggregation. Also, when there are no DISTINCT
// aggregations best-effort preaggregation may manage to fully aggregate the
// input - in this case sorting and final aggregation is not needed (which
// improves performance).
//
// The preaggregated data is then sorted on the extended key and
// AggregateClusters is performed to combine rows with equal keys.  In the last
// step final aggregation is performed with AggregateClusters on the original
// key.

// Analyzes grouping specification and sets up parameters for various stages of
// hybrid group implementation.
class HybridGroupSetup {
 public:
  // Takes ownership of input.
  static FailureOrOwned<const HybridGroupSetup> Create(
      const SingleSourceProjector& group_by_columns,
      const AggregationSpecification& aggregation_specification,
      const TupleSchema& input_schema) {
    scoped_ptr<HybridGroupSetup> setup(new HybridGroupSetup);
    PROPAGATE_ON_FAILURE(setup->Init(group_by_columns,
                                     aggregation_specification,
                                     input_schema));
    return Success(setup.release());
  }

  // For restoring non-nullability of COUNT columns in hybrid group by. We
  // combine partial COUNT results using SUM and SUM's result is nullable.
  // TODO(user): This would be unneccesary if we could specify that SUM's result
  // should be not nullable.
  FailureOrOwned<Cursor> MakeCountColumnsNotNullable(
      Cursor* cursor,
      BufferAllocator* allocator) const {
    CHECK(initialized_);
    vector<string> column_names(columns_to_make_not_nullable_.begin(),
                                columns_to_make_not_nullable_.end());
    return column_names.empty()
        ? Success(cursor)
        : MakeSelectedColumnsNotNullable(ProjectNamedAttributes(column_names),
                                         allocator, cursor);
  }

  // Computes the result schema for HybridGroupFinalAggregationCursor.
  FailureOr<TupleSchema> ComputeResultSchema(
      const TupleSchema& group_by_columns_schema,
      const TupleSchema& aggregator_schema) const {
    CHECK(initialized_);
    FailureOr<TupleSchema> result_schema_bad_nullability =
        TupleSchema::TryMerge(group_by_columns_schema, aggregator_schema);
    PROPAGATE_ON_FAILURE(result_schema_bad_nullability);
    // Fixing nullability of COUNT columns. They may be wrong in
    // final_aggregator->schema() because final_aggregation uses SUM to compute
    // final results of COUNT(column) and SUMS's result is nullable.
    TupleSchema result_schema;
    for (int i = 0; i < result_schema_bad_nullability.get().attribute_count();
         ++i) {
      const Attribute& attribute =
          result_schema_bad_nullability.get().attribute(i);
      result_schema.add_attribute(
          Attribute(attribute.name(),
                    attribute.type(),
                    ContainsKey(columns_to_make_not_nullable_, attribute.name())
                        ? NOT_NULLABLE
                        : attribute.nullability()));
    }
    return Success(result_schema);
  }

  // Returns a cursor with input transformed for hybrid group.
  FailureOrOwned<Cursor> TransformInput(BufferAllocator* allocator,
                                        Cursor* input) const {
    CHECK(initialized_);
    scoped_ptr<Cursor> input_owned(input);
    if (count_star_present_) {
      // Add a column with non-null values for implementing COUNT(*).
      FailureOrOwned<Cursor> input_with_count_star(
          ExtendByConstantColumn(count_star_column_name_, allocator,
                                 input_owned.release()));
      PROPAGATE_ON_FAILURE(input_with_count_star);
      input_owned.reset(input_with_count_star.release());
    }
    return BoundHybridGroupTransform(
        group_by_columns_->Clone(),
        column_group_projectors_,
        allocator,
        input_owned.release());
  }

  const AggregationSpecification& pregroup_aggregation() const {
    CHECK(initialized_);
    return pregroup_aggregation_;
  }
  const AggregationSpecification& pregroup_combine_aggregation() const {
    CHECK(initialized_);
    return pregroup_combine_aggregation_;
  }
  const AggregationSpecification& final_aggregation() const {
    CHECK(initialized_);
    return final_aggregation_;
  }
  const SingleSourceProjector& pregroup_group_by_columns() const {
    CHECK(initialized_);
    return pregroup_group_by_columns_;
  }

  bool has_distinct_aggregations() const {
    CHECK(initialized_);
    return has_distinct_aggregations_;
  }
  const SingleSourceProjector& group_by_columns_by_name() const {
    CHECK(initialized_);
    return *group_by_columns_by_name_;
  }

 private:
  HybridGroupSetup()
      : initialized_(false),
        count_star_column_name_("$hybrid_group_count_star$") {}

  FailureOrVoid Init(
      const SingleSourceProjector& group_by_columns,
      const AggregationSpecification& aggregation_specification,
      const TupleSchema& input_schema) {
    CHECK(!initialized_);
    group_by_columns_.reset(group_by_columns.Clone());
    has_distinct_aggregations_ = false;
    count_star_present_ = false;
    // Hybrid group-by may need to project group-by columns multiple times
    // over transformed inputs, and it wouldn't work if the projector was
    // renaming columns, selecting columns by position or taking all input
    // columns. Because of this, in later stages of processing we use a
    // projector based on result column names of the original projector.
    FailureOrOwned<const SingleSourceProjector> group_by_columns_by_name(
        ProjectUsingProjectorResultNames(group_by_columns, input_schema));
    PROPAGATE_ON_FAILURE(group_by_columns_by_name);
    group_by_columns_by_name_.reset(group_by_columns_by_name.release());
    // pregroup_group_by_columns_ will contain all the original group-by columns
    // plus all the distinct aggregated columns.
    pregroup_group_by_columns_.add(group_by_columns_by_name_->Clone());
    set<string> distinct_columns_set;
    set<string> nondistinct_columns_set;
    CompoundSingleSourceProjector nondistinct_columns;
    for (int i = 0; i < aggregation_specification.size(); ++i) {
      const AggregationSpecification::Element& elem =
          aggregation_specification.aggregation(i);
      const string pregroup_column_prefix =
          elem.is_distinct()
            ? "$hybrid_group_pregroup_column_d$"
            : "$hybrid_group_pregroup_column_nd$";
      const string pregroup_column_name = StrCat(pregroup_column_prefix,
                                                 elem.input());
      AggregationSpecification::Element final_elem(elem);
      if (elem.is_distinct()) {
        has_distinct_aggregations_ = true;
        // <aggregate-function>(DISTINCT col).
        if (elem.input() == "") {
          THROW(new Exception(
              ERROR_ATTRIBUTE_MISSING,
              StringPrintf("Incorrect aggregation specification. Distinct "
                           "aggregation needs input column.")));
        }
        if (InsertIfNotPresent(&distinct_columns_set, elem.input())) {
          column_group_projectors_.push_back(
              ProjectNamedAttributeAs(elem.input(),
                                      pregroup_column_name));
          pregroup_group_by_columns_.add(
              ProjectNamedAttributeAs(pregroup_column_name,
                                      pregroup_column_name));
        }
        // No need to add anything to pregroup_aggregation_.
        final_elem.set_input(pregroup_column_name);
      } else {
        AggregationSpecification::Element pregroup_elem(elem);
        // <aggregate-function>(col) or COUNT(*).
        if (elem.input() != "") {
          if (InsertIfNotPresent(&nondistinct_columns_set, elem.input())) {
            nondistinct_columns.add(
                ProjectNamedAttributeAs(elem.input(),
                                        pregroup_column_name));
          }
          pregroup_elem.set_input(pregroup_column_name);
        } else {
          count_star_present_ = true;
          pregroup_elem.set_input(count_star_column_name_);
        }
        final_elem.set_input(pregroup_elem.output());
        if (elem.aggregation_operator() == COUNT) {
          columns_to_make_not_nullable_.insert(final_elem.output());
          final_elem.set_aggregation_operator(SUM);
        }
        AggregationSpecification::Element pregroup_combine_elem(final_elem);
        pregroup_combine_elem.set_output(pregroup_combine_elem.input());
        pregroup_aggregation_.add(pregroup_elem);
        pregroup_combine_aggregation_.add(pregroup_combine_elem);
      }
      final_aggregation_.add(final_elem);
    }
    if (count_star_present_) {
      // Add a column with non-null values for implementing COUNT(*).
      nondistinct_columns.add(
          ProjectNamedAttributeAs(count_star_column_name_,
                                  count_star_column_name_));
    }
    if (!nondistinct_columns_set.empty() || count_star_present_ ||
        column_group_projectors_.empty()) {
      column_group_projectors_.push_back(nondistinct_columns.Clone());
    }
    initialized_ = true;
    return Success();
  }

  static FailureOrOwned<const SingleSourceProjector>
      ProjectUsingProjectorResultNames(const SingleSourceProjector& projector,
                                       const TupleSchema& schema) {
    FailureOrOwned<const BoundSingleSourceProjector> bound_projector(
        projector.Bind(schema));
    PROPAGATE_ON_FAILURE(bound_projector);
    const TupleSchema& result_schema = bound_projector->result_schema();
    vector<string> result_names;
    for (int i = 0; i < result_schema.attribute_count(); ++i) {
      result_names.push_back(result_schema.attribute(i).name());
    }
    return Success(ProjectNamedAttributes(result_names));
  }

 private:
  // Was the object initialized successfully with Init(...).
  bool initialized_;

  // Does the AggregationSpecification contain DISTINCT aggregations?
  bool has_distinct_aggregations_;

  // Does the AggregationSpecification contain COUNT(*)?
  bool count_star_present_;

  // The name of the column added for COUNT(*) implementation.
  const string count_star_column_name_;

  // The original group by columns projector.
  scoped_ptr<const SingleSourceProjector> group_by_columns_;

  // A safe projector for use in later processing, based on the result
  // names of the original group_by_columns_ projector.
  scoped_ptr<const SingleSourceProjector> group_by_columns_by_name_;

  // A set of COUNT columns (names) that will need nullability fixing because
  // the implementation uses SUM on later (post pregroup) stages.
  set<string> columns_to_make_not_nullable_;

  // A set of projectors that define column groups for the
  // HybridGroupTransformCursor. There's a separate projector for each column
  // that has DISTINCT aggregations on it, and a shared projector for all
  // columns that have non-DISTINCT aggregations on them.
  util::gtl::PointerVector<const SingleSourceProjector>
      column_group_projectors_;

  // Group-by columns for the pregroup aggregation (will additionally contain
  // distinct aggregated columns).
  CompoundSingleSourceProjector pregroup_group_by_columns_;

  // The initial (pregroup) aggregation.
  AggregationSpecification pregroup_aggregation_;

  // Aggregation for combining results of pregroup for duplicated keys.
  AggregationSpecification pregroup_combine_aggregation_;

  // Final aggregation combining results of pregroup aggregation for
  // non-DISTINCT aggregations and using ordinary, non-DISTINCT aggregations in
  // place of original DISTINCT aggregations.
  AggregationSpecification final_aggregation_;

  DISALLOW_COPY_AND_ASSIGN(HybridGroupSetup);
};

// Combines preaggregated results on repeated (extended) keys and the final
// aggregation to compute the results for DISTINCT aggregations.
class HybridGroupFinalAggregationCursor : public BasicCursor {
 public:
  // Takes ownership of hybrid_group_setup, final_aggregator,
  // final_group_by_columns and pregroup_cursor.
  HybridGroupFinalAggregationCursor(
      const TupleSchema& result_schema,
      const HybridGroupSetup* hybrid_group_setup,
      Aggregator* final_aggregator,
      const BoundSingleSourceProjector* final_group_by_columns,
      StringPiece temporary_directory_prefix,
      BufferAllocator* allocator,
      GroupAggregateCursor* pregroup_cursor)
      : BasicCursor(result_schema),
        is_waiting_on_barrier_supported_(
            pregroup_cursor->IsWaitingOnBarrierSupported()),
        hybrid_group_setup_(hybrid_group_setup),
        final_aggregator_(final_aggregator),
        final_group_by_columns_(final_group_by_columns),
        temporary_directory_prefix_(temporary_directory_prefix.ToString()),
        allocator_(allocator),
        pregroup_cursor_(pregroup_cursor) {
  }

  virtual ResultView Next(rowcount_t max_row_count) {
    if (result_cursor_ == NULL && sorter_ == NULL) {
      ResultView first_result = pregroup_cursor_->Next(
          numeric_limits<rowcount_t>::max());
      if (first_result.is_eos() || !first_result.has_data()) {
        return first_result;
      }
      if (!pregroup_cursor_->CanReturnMoreData() &&
          !hybrid_group_setup_->has_distinct_aggregations()) {
        pregroup_cursor_->TruncateResultView();
        // There are no distinct aggregations, so if pregroup fully aggregated
        // all the data (a single output block), its result can be returned as
        // final result.
        LOG(INFO) << "HybridGroupAggregate not using disk.";
        PROPAGATE_ON_FAILURE(SetResultCursor(pregroup_cursor_.release()));
      } else {
        // Pregroup best-effort didn't fully aggregate the data. Sorting data to
        // combine duplicate pregroup keys. The final aggregation will be
        // performed using AggregateClusters, as the data will also be sorted on
        // the original key.
        if (first_result.has_data()) {
          pregroup_cursor_->TruncateResultView();
        }
        LOG(INFO) << "HybridGroupAggregate using disk ("
                  << (hybrid_group_setup_->has_distinct_aggregations()
                      ? "distinct aggregations" : "best-effort failed")
                  << ").";
        FailureOrOwned<const BoundSingleSourceProjector> bound_group_by_columns(
            hybrid_group_setup_->pregroup_group_by_columns().Bind(
                pregroup_cursor_->schema()));
        PROPAGATE_ON_FAILURE(bound_group_by_columns);
        sorter_.reset(CreateUnbufferedSorter(
            pregroup_cursor_->schema(),
            new BoundSortOrder(bound_group_by_columns.release()),
            temporary_directory_prefix_,
            allocator_));
      }
    }
    if (result_cursor_ == NULL && sorter_ != NULL) {
      while (true) {
        ResultView result = pregroup_cursor_->Next(
            numeric_limits<rowcount_t>::max());
        PROPAGATE_ON_FAILURE(result);
        if (result.is_waiting_on_barrier()) {
          return ResultView::WaitingOnBarrier();
        }
        if (result.is_eos()) {
          break;
        }
        VLOG(1) << "Writing " << result.view().row_count()
                << " rows to UnbufferedSorter.";
        FailureOr<rowcount_t> written = sorter_->Write(result.view());
        PROPAGATE_ON_FAILURE(written);
        CHECK_EQ(written.get(), result.view().row_count());
      }
      // Aggregate to combine duplicated pregroup keys.
      FailureOrOwned<Cursor> sorted = sorter_->GetResultCursor();
      PROPAGATE_ON_FAILURE(sorted);
      sorter_.reset();
      FailureOrOwned<Aggregator> aggregator = Aggregator::Create(
          hybrid_group_setup_->pregroup_combine_aggregation(),
          sorted.get()->schema(),
          allocator_,
          1024);
      PROPAGATE_ON_FAILURE(aggregator);
      FailureOrOwned<const BoundSingleSourceProjector> sorted_group_by_columns =
          hybrid_group_setup_->pregroup_group_by_columns().Bind(
              sorted.get()->schema());
      PROPAGATE_ON_FAILURE(sorted_group_by_columns);
      FailureOrOwned<Cursor> combine_cursor =
          BoundAggregateClusters(
              sorted_group_by_columns.release(),
              aggregator.release(),
              allocator_,
              sorted.release());
      PROPAGATE_ON_FAILURE(combine_cursor);
      // Restoring COUNT column nullability, so we have exactly the same schema
      // that final_aggregator was built for.
      FailureOrOwned<Cursor> combine_cursor_count_nonnullable =
          hybrid_group_setup_->MakeCountColumnsNotNullable(
              combine_cursor.release(), allocator_);
      PROPAGATE_ON_FAILURE(combine_cursor_count_nonnullable);
      // Final aggregation on the original key using AggregateClusters.
      FailureOrOwned<Cursor> aggregated = BoundAggregateClusters(
          final_group_by_columns_.release(),
          final_aggregator_.release(),
          allocator_,
          combine_cursor_count_nonnullable.release());
      PROPAGATE_ON_FAILURE(aggregated);
      PROPAGATE_ON_FAILURE(SetResultCursor(aggregated.release()));
    }
    CHECK(sorter_ == NULL);
    CHECK(result_cursor_ != NULL);
    ResultView result = result_cursor_->Next(max_row_count);
    PROPAGATE_ON_FAILURE(result);
    return result;
  }

  virtual bool IsWaitingOnBarrierSupported() const {
    return is_waiting_on_barrier_supported_;
  }

  virtual void Interrupt() {
    Cursor* pregroup_cursor = pregroup_cursor_.get();
    if (pregroup_cursor != NULL) {
      pregroup_cursor->Interrupt();
    }
    Cursor* result_cursor = result_cursor_.get();
    if (result_cursor != NULL) {
      result_cursor->Interrupt();
    }
  }

  virtual CursorId GetCursorId() const {
    return HYBRID_GROUP_FINAL_AGGREGATION;
  }

  // Runs the transformer recursively on the pregrouping aggregate cursor.
  //
  // This solution effectively omits the transformation of the pregrouping
  // cursor, as it conflicts with the return type of Transform().
  // The hybrid aggregate and pregroup aggregate cursor are treated as one
  // by cursor transformers.
  virtual void ApplyToChildren(CursorTransformer* transformer) {
    pregroup_cursor_->ApplyToChildren(transformer);
  }

 private:
  FailureOrVoid SetResultCursor(Cursor* result_cursor) {
    FailureOrOwned<Cursor> result_cursor_fixed_nullability =
        hybrid_group_setup_->MakeCountColumnsNotNullable(
            result_cursor, allocator_);
    PROPAGATE_ON_FAILURE(result_cursor_fixed_nullability);
    result_cursor_.reset(result_cursor_fixed_nullability.release());
    CHECK(TupleSchema::AreEqual(schema(), result_cursor_->schema(), true))
        << "schema(): "
        << schema().GetHumanReadableSpecification()
        << "\naggregated_owned->schema(): "
        << result_cursor_->schema().GetHumanReadableSpecification();
    return Success();
  }

  bool is_waiting_on_barrier_supported_;
  scoped_ptr<const HybridGroupSetup> hybrid_group_setup_;
  scoped_ptr<Aggregator> final_aggregator_;
  scoped_ptr<const BoundSingleSourceProjector> final_group_by_columns_;
  size_t memory_quota_;
  string temporary_directory_prefix_;
  BufferAllocator* allocator_;
  scoped_ptr<GroupAggregateCursor> pregroup_cursor_;
  scoped_ptr<Sorter> sorter_;
  scoped_ptr<Cursor> result_cursor_;
  DISALLOW_COPY_AND_ASSIGN(HybridGroupFinalAggregationCursor);
};

Operation* GroupAggregate(
    const SingleSourceProjector* group_by,
    const AggregationSpecification* aggregation,
    GroupAggregateOptions* options,
    Operation* child) {
  return new GroupAggregateOperation(group_by, aggregation, options, false,
                                     child);
}

Operation* BestEffortGroupAggregate(
    const SingleSourceProjector* group_by,
    const AggregationSpecification* aggregation,
    GroupAggregateOptions* options,
    Operation* child) {
  return new GroupAggregateOperation(group_by, aggregation, options, true,
                                     child);
}

FailureOrOwned<Cursor> BoundGroupAggregate(
    const BoundSingleSourceProjector* group_by,
        Aggregator* aggregator,
        BufferAllocator* allocator,
        BufferAllocator* original_allocator,
        bool best_effort,
        Cursor* child) {
  FailureOrOwned<GroupAggregateCursor> result =
      GroupAggregateCursor::Create(
          group_by, aggregator, allocator,
          original_allocator == NULL ? allocator : original_allocator,
          best_effort, child);
  PROPAGATE_ON_FAILURE(result);
  return Success(result.release());
}

FailureOrOwned<Cursor> BoundHybridGroupAggregate(
    const SingleSourceProjector* group_by_columns,
    const AggregationSpecification& aggregation_specification,
    StringPiece temporary_directory_prefix,
    BufferAllocator* allocator,
    size_t memory_quota,
    const HybridGroupDebugOptions* debug_options,
    Cursor* child) {
  scoped_ptr<Cursor> child_owner(child);
  scoped_ptr<const SingleSourceProjector> group_by_columns_owner(
      group_by_columns);
  scoped_ptr<const HybridGroupDebugOptions> debug_options_owned(debug_options);
  FailureOrOwned<const HybridGroupSetup> hybrid_group_setup(
      HybridGroupSetup::Create(*group_by_columns_owner,
                               aggregation_specification,
                               child_owner->schema()));
  PROPAGATE_ON_FAILURE(hybrid_group_setup);
  FailureOrOwned<Cursor> transformed_input(
      hybrid_group_setup->TransformInput(allocator, child_owner.release()));
  PROPAGATE_ON_FAILURE(transformed_input);
  if (debug_options_owned != NULL &&
      debug_options_owned->return_transformed_input()) {
    return transformed_input;
  }
  // Use best-effort group aggregate to pregroup the data.
  FailureOrOwned<const BoundSingleSourceProjector> bound_pregroup_columns(
      hybrid_group_setup->pregroup_group_by_columns().Bind(
          transformed_input->schema()));
  PROPAGATE_ON_FAILURE(bound_pregroup_columns);
  // limit_allocator will be owned by 'pregroup_cursor'.
  scoped_ptr<BufferAllocator> limit_allocator(
      new MemoryLimit(memory_quota, false, allocator));

  FailureOrOwned<Aggregator> pregroup_aggregator = Aggregator::Create(
      hybrid_group_setup->pregroup_aggregation(), transformed_input->schema(),
      limit_allocator.get(),
      1024);
  PROPAGATE_ON_FAILURE(pregroup_aggregator);
  FailureOrOwned<GroupAggregateCursor> pregroup_cursor =
      GroupAggregateCursor::Create(
          bound_pregroup_columns.release(),
          pregroup_aggregator.release(),
          limit_allocator.release(),
          allocator,
          true,  // best effort.
          transformed_input.release());
  PROPAGATE_ON_FAILURE(pregroup_cursor);
  // Building final_aggregator and final_group_by_columns to compute the
  // result_schema for HybridGroupFinalAggregationCursor.
  FailureOrOwned<Aggregator> final_aggregator = Aggregator::Create(
      hybrid_group_setup->final_aggregation(),
      pregroup_cursor->schema(),
      allocator,
      1024);
  PROPAGATE_ON_FAILURE(final_aggregator);
  FailureOrOwned<const BoundSingleSourceProjector> final_group_by_columns(
      hybrid_group_setup->group_by_columns_by_name().Bind(
          pregroup_cursor->schema()));
  PROPAGATE_ON_FAILURE(final_group_by_columns);
  // Calculate the schema of the final cursor.
  FailureOr<TupleSchema> result_schema =
      hybrid_group_setup->ComputeResultSchema(
          final_group_by_columns->result_schema(),
          final_aggregator->schema());
  PROPAGATE_ON_FAILURE(result_schema);
  return Success(new HybridGroupFinalAggregationCursor(
      result_schema.get(),
      hybrid_group_setup.release(),
      final_aggregator.release(),
      final_group_by_columns.release(),
      temporary_directory_prefix,
      allocator,
      pregroup_cursor.release()));
}

class HybridGroupAggregateOperation : public BasicOperation {
 public:
  HybridGroupAggregateOperation(
      const SingleSourceProjector* group_by_columns,
      const AggregationSpecification* aggregation_specification,
      size_t memory_quota,
      StringPiece temporary_directory_prefix,
      Operation* child)
      : BasicOperation(child),
        group_by_columns_(group_by_columns),
        aggregation_specification_(aggregation_specification),
        memory_quota_(memory_quota),
        temporary_directory_prefix_(temporary_directory_prefix.ToString()) {}

  virtual ~HybridGroupAggregateOperation() {}

  virtual FailureOrOwned<Cursor> CreateCursor() const {
    FailureOrOwned<Cursor> child_cursor = child()->CreateCursor();
    PROPAGATE_ON_FAILURE(child_cursor);
    return BoundHybridGroupAggregate(
        group_by_columns_->Clone(),
        *aggregation_specification_,
        temporary_directory_prefix_,
        buffer_allocator(),
        memory_quota_,
        NULL,
        child_cursor.release());
  }

 private:
  scoped_ptr<const SingleSourceProjector> group_by_columns_;
  scoped_ptr<const AggregationSpecification> aggregation_specification_;
  size_t memory_quota_;
  string temporary_directory_prefix_;
  DISALLOW_COPY_AND_ASSIGN(HybridGroupAggregateOperation);
};

Operation* HybridGroupAggregate(
    const SingleSourceProjector* group_by_columns,
    const AggregationSpecification* aggregation_specification,
    size_t memory_quota,
    StringPiece temporary_directory_prefix,
    Operation* child) {
  return new HybridGroupAggregateOperation(group_by_columns,
                                           aggregation_specification,
                                           memory_quota,
                                           temporary_directory_prefix,
                                           child);
}

}  // namespace supersonic
